{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUnAZVZ_UF3g"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "source_folder = '/content/drive/MyDrive/Dataset/Morteza/Data/Resnet/DataMain/MortezaRawImages'\n",
    "metal_indicator_lung = '/content/drive/MyDrive/Dataset/Morteza/Data/Resnet/Test/Metals_Indicators.xlsx'\n",
    "keyFileNameCorr = '/content/drive/MyDrive/Dataset/Morteza/Data/Resnet/Test/Lung1-7.xlsx'\n",
    "clinical_var_address = '/content/drive/MyDrive/Dataset/Morteza/Data/Resnet/Test/Variables/progressNotProgres.xlsx'\n",
    "dir_experiment = '/content/drive/MyDrive/Dataset/Morteza/Data/Resnet/Experiments/'\n",
    "experiment_name = 'pred_progress_v_0'\n",
    "channel_names = ['aSMA','Arg1','B7-H3','B7-H4','BCL2','CD3','CD4','CD8a','CD11c','CD14','CD16','CD20','CD31','CD39','CD40','CD45','CD68','CD94','CD117','CD163','Cleaved Caspase3','DNA1','DNA2','FoxP3','HIF-1a','Histone H3','HLA-DR','Ki67','MCSFR','MMP9','MPO','Pancytokeratin','PD1','PDL1-Biotin','pERK','pSTAT3','TTF1']\n",
    "RAW_Image_Source_Folder = '/content/drive/MyDrive/Dataset/Morteza/Data/Resnet/DataMain/MortezaRawImages/'\n",
    "num_classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7msBeF85wX_s"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "resnet_models = {18: torchvision.models.resnet18,\n",
    "                 34: torchvision.models.resnet34,\n",
    "                 50: torchvision.models.resnet50,\n",
    "                 101: torchvision.models.resnet101,\n",
    "                 152: torchvision.models.resnet152}\n",
    "\n",
    "class Resnet_multichannel(nn.Module):\n",
    "    def __init__(self, pretrained=True, encoder_depth=34, num_in_channels=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        if encoder_depth not in [18, 34, 50, 101, 152]:\n",
    "            raise ValueError(f\"Encoder depth {encoder_depth} specified does not match any existing Resnet models\")\n",
    "            \n",
    "        model = resnet_models[encoder_depth](pretrained)\n",
    "        \n",
    "        ##For reference: layers to use (in order):\n",
    "        # conv1, bn1, relu, maxpool, layer1, layer2, layer3, layer4, avgpool, fc\n",
    "        \n",
    "        # This is the most important line of code here. This increases the number of in channels for our network\n",
    "        self.conv1 = self.increase_channels(model.conv1, num_in_channels)\n",
    "        \n",
    "        self.bn1 = model.bn1\n",
    "        self.relu = model.relu\n",
    "        self.maxpool = model.maxpool\n",
    "        self.layer1 = model.layer1\n",
    "        self.layer2 = model.layer2\n",
    "        self.layer3 = model.layer3\n",
    "        self.layer4 = model.layer4\n",
    "        self.avgpool = model.avgpool\n",
    "        self.fc = model.fc\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "    def increase_channels(self, m, num_channels=None, copy_weights=0):\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        takes as input a Conv2d layer and returns the a Conv2d layer with `num_channels` input channels\n",
    "        and all the previous weights copied into the new layer.\n",
    "        \"\"\"\n",
    "        # number of input channels the new module should have\n",
    "        new_in_channels = num_channels if num_channels is not None else m.in_channels + 1\n",
    "        \n",
    "        bias = False if m.bias is None else True\n",
    "        \n",
    "        # Creating new Conv2d layer\n",
    "        new_m = nn.Conv2d(in_channels=new_in_channels, \n",
    "                          out_channels=m.out_channels, \n",
    "                          kernel_size=m.kernel_size, \n",
    "                          stride=m.stride, \n",
    "                          padding=m.padding,\n",
    "                          bias=bias)\n",
    "        \n",
    "        # Copying the weights from the old to the new layer\n",
    "        new_m.weight.data[:, :m.in_channels, :, :] = m.weight.clone()\n",
    "        \n",
    "        #Copying the weights of the `copy_weights` channel of the old layer to the extra channels of the new layer\n",
    "        for i in range(new_in_channels - m.in_channels):\n",
    "            channel = m.in_channels + i\n",
    "            new_m.weight.data[:, channel:channel+1, :, :] = m.weight[:, copy_weights:copy_weights+1, : :].clone()\n",
    "        new_m.weight = nn.Parameter(new_m.weight)\n",
    "\n",
    "        return new_m\n",
    "    \n",
    "def get_arch(encoder_depth, num_in_channels):\n",
    "    \"\"\"\n",
    "    Returns just an architecture which can then be called in the usual way.\n",
    "    For example:\n",
    "    resnet34_4_channel = get_arch(34, 4)\n",
    "    model = resnet34_4_channel(True)\n",
    "    \"\"\"\n",
    "    return partial(Resnet_multichannel, encoder_depth=encoder_depth, num_in_channels=num_in_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "a1b5d9e0a968445ab02188add8760207",
      "bb226bb996a4492a93a96cba86e9ed13",
      "56990f43294f4b2dabd7a79c1d16ceb6",
      "78ee2f1b15fa41498075e1f58a5f1223",
      "b356474b77184c14b5002d3f27cc7317",
      "d1b8b051dc944a5da7768a4fb409ce7c",
      "25ea356dcdc9432eb8ead852c09391cc",
      "3481fd4957a84b3c950e7483d33e2ec2",
      "55ecccbf86e3433e89a75b5b810e8a30",
      "924cb40c28674d818e0048c266755f70",
      "9f572ff01bea48a8803018f146269d74"
     ]
    },
    "id": "Fe-gvXX1wa7h",
    "outputId": "4da9b502-9d77-45c9-c784-3d15b8abcdf3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b5d9e0a968445ab02188add8760207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New input channels :  3000\n"
     ]
    }
   ],
   "source": [
    "resnet50_mchannel = get_arch(50, 3000)\n",
    "model = resnet50_mchannel(True) \n",
    "print(\"New input channels : \", model.conv1.in_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAG_iiJQxYxF",
    "outputId": "a36acc9a-e2f0-4456-f879-2b2ea384af49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2WJgIJ2n_VL"
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2mAIb0UoFND"
   },
   "outputs": [],
   "source": [
    "feature_extract = True\n",
    "set_parameter_requires_grad(model, feature_extract)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Airw0XJRoa1B",
    "outputId": "0687d6ed-73aa-4498-a135-e840bbb99013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "Resnet_multichannel                      --\n",
      "├─Conv2d: 1-1                            (9,408,000)\n",
      "├─BatchNorm2d: 1-2                       (128)\n",
      "├─ReLU: 1-3                              --\n",
      "├─MaxPool2d: 1-4                         --\n",
      "├─Sequential: 1-5                        --\n",
      "│    └─Bottleneck: 2-1                   --\n",
      "│    │    └─Conv2d: 3-1                  (4,096)\n",
      "│    │    └─BatchNorm2d: 3-2             (128)\n",
      "│    │    └─Conv2d: 3-3                  (36,864)\n",
      "│    │    └─BatchNorm2d: 3-4             (128)\n",
      "│    │    └─Conv2d: 3-5                  (16,384)\n",
      "│    │    └─BatchNorm2d: 3-6             (512)\n",
      "│    │    └─ReLU: 3-7                    --\n",
      "│    │    └─Sequential: 3-8              (16,896)\n",
      "│    └─Bottleneck: 2-2                   --\n",
      "│    │    └─Conv2d: 3-9                  (16,384)\n",
      "│    │    └─BatchNorm2d: 3-10            (128)\n",
      "│    │    └─Conv2d: 3-11                 (36,864)\n",
      "│    │    └─BatchNorm2d: 3-12            (128)\n",
      "│    │    └─Conv2d: 3-13                 (16,384)\n",
      "│    │    └─BatchNorm2d: 3-14            (512)\n",
      "│    │    └─ReLU: 3-15                   --\n",
      "│    └─Bottleneck: 2-3                   --\n",
      "│    │    └─Conv2d: 3-16                 (16,384)\n",
      "│    │    └─BatchNorm2d: 3-17            (128)\n",
      "│    │    └─Conv2d: 3-18                 (36,864)\n",
      "│    │    └─BatchNorm2d: 3-19            (128)\n",
      "│    │    └─Conv2d: 3-20                 (16,384)\n",
      "│    │    └─BatchNorm2d: 3-21            (512)\n",
      "│    │    └─ReLU: 3-22                   --\n",
      "├─Sequential: 1-6                        --\n",
      "│    └─Bottleneck: 2-4                   --\n",
      "│    │    └─Conv2d: 3-23                 (32,768)\n",
      "│    │    └─BatchNorm2d: 3-24            (256)\n",
      "│    │    └─Conv2d: 3-25                 (147,456)\n",
      "│    │    └─BatchNorm2d: 3-26            (256)\n",
      "│    │    └─Conv2d: 3-27                 (65,536)\n",
      "│    │    └─BatchNorm2d: 3-28            (1,024)\n",
      "│    │    └─ReLU: 3-29                   --\n",
      "│    │    └─Sequential: 3-30             (132,096)\n",
      "│    └─Bottleneck: 2-5                   --\n",
      "│    │    └─Conv2d: 3-31                 (65,536)\n",
      "│    │    └─BatchNorm2d: 3-32            (256)\n",
      "│    │    └─Conv2d: 3-33                 (147,456)\n",
      "│    │    └─BatchNorm2d: 3-34            (256)\n",
      "│    │    └─Conv2d: 3-35                 (65,536)\n",
      "│    │    └─BatchNorm2d: 3-36            (1,024)\n",
      "│    │    └─ReLU: 3-37                   --\n",
      "│    └─Bottleneck: 2-6                   --\n",
      "│    │    └─Conv2d: 3-38                 (65,536)\n",
      "│    │    └─BatchNorm2d: 3-39            (256)\n",
      "│    │    └─Conv2d: 3-40                 (147,456)\n",
      "│    │    └─BatchNorm2d: 3-41            (256)\n",
      "│    │    └─Conv2d: 3-42                 (65,536)\n",
      "│    │    └─BatchNorm2d: 3-43            (1,024)\n",
      "│    │    └─ReLU: 3-44                   --\n",
      "│    └─Bottleneck: 2-7                   --\n",
      "│    │    └─Conv2d: 3-45                 (65,536)\n",
      "│    │    └─BatchNorm2d: 3-46            (256)\n",
      "│    │    └─Conv2d: 3-47                 (147,456)\n",
      "│    │    └─BatchNorm2d: 3-48            (256)\n",
      "│    │    └─Conv2d: 3-49                 (65,536)\n",
      "│    │    └─BatchNorm2d: 3-50            (1,024)\n",
      "│    │    └─ReLU: 3-51                   --\n",
      "├─Sequential: 1-7                        --\n",
      "│    └─Bottleneck: 2-8                   --\n",
      "│    │    └─Conv2d: 3-52                 (131,072)\n",
      "│    │    └─BatchNorm2d: 3-53            (512)\n",
      "│    │    └─Conv2d: 3-54                 (589,824)\n",
      "│    │    └─BatchNorm2d: 3-55            (512)\n",
      "│    │    └─Conv2d: 3-56                 (262,144)\n",
      "│    │    └─BatchNorm2d: 3-57            (2,048)\n",
      "│    │    └─ReLU: 3-58                   --\n",
      "│    │    └─Sequential: 3-59             (526,336)\n",
      "│    └─Bottleneck: 2-9                   --\n",
      "│    │    └─Conv2d: 3-60                 (262,144)\n",
      "│    │    └─BatchNorm2d: 3-61            (512)\n",
      "│    │    └─Conv2d: 3-62                 (589,824)\n",
      "│    │    └─BatchNorm2d: 3-63            (512)\n",
      "│    │    └─Conv2d: 3-64                 (262,144)\n",
      "│    │    └─BatchNorm2d: 3-65            (2,048)\n",
      "│    │    └─ReLU: 3-66                   --\n",
      "│    └─Bottleneck: 2-10                  --\n",
      "│    │    └─Conv2d: 3-67                 (262,144)\n",
      "│    │    └─BatchNorm2d: 3-68            (512)\n",
      "│    │    └─Conv2d: 3-69                 (589,824)\n",
      "│    │    └─BatchNorm2d: 3-70            (512)\n",
      "│    │    └─Conv2d: 3-71                 (262,144)\n",
      "│    │    └─BatchNorm2d: 3-72            (2,048)\n",
      "│    │    └─ReLU: 3-73                   --\n",
      "│    └─Bottleneck: 2-11                  --\n",
      "│    │    └─Conv2d: 3-74                 (262,144)\n",
      "│    │    └─BatchNorm2d: 3-75            (512)\n",
      "│    │    └─Conv2d: 3-76                 (589,824)\n",
      "│    │    └─BatchNorm2d: 3-77            (512)\n",
      "│    │    └─Conv2d: 3-78                 (262,144)\n",
      "│    │    └─BatchNorm2d: 3-79            (2,048)\n",
      "│    │    └─ReLU: 3-80                   --\n",
      "│    └─Bottleneck: 2-12                  --\n",
      "│    │    └─Conv2d: 3-81                 (262,144)\n",
      "│    │    └─BatchNorm2d: 3-82            (512)\n",
      "│    │    └─Conv2d: 3-83                 (589,824)\n",
      "│    │    └─BatchNorm2d: 3-84            (512)\n",
      "│    │    └─Conv2d: 3-85                 (262,144)\n",
      "│    │    └─BatchNorm2d: 3-86            (2,048)\n",
      "│    │    └─ReLU: 3-87                   --\n",
      "│    └─Bottleneck: 2-13                  --\n",
      "│    │    └─Conv2d: 3-88                 (262,144)\n",
      "│    │    └─BatchNorm2d: 3-89            (512)\n",
      "│    │    └─Conv2d: 3-90                 (589,824)\n",
      "│    │    └─BatchNorm2d: 3-91            (512)\n",
      "│    │    └─Conv2d: 3-92                 (262,144)\n",
      "│    │    └─BatchNorm2d: 3-93            (2,048)\n",
      "│    │    └─ReLU: 3-94                   --\n",
      "├─Sequential: 1-8                        --\n",
      "│    └─Bottleneck: 2-14                  --\n",
      "│    │    └─Conv2d: 3-95                 (524,288)\n",
      "│    │    └─BatchNorm2d: 3-96            (1,024)\n",
      "│    │    └─Conv2d: 3-97                 (2,359,296)\n",
      "│    │    └─BatchNorm2d: 3-98            (1,024)\n",
      "│    │    └─Conv2d: 3-99                 (1,048,576)\n",
      "│    │    └─BatchNorm2d: 3-100           (4,096)\n",
      "│    │    └─ReLU: 3-101                  --\n",
      "│    │    └─Sequential: 3-102            (2,101,248)\n",
      "│    └─Bottleneck: 2-15                  --\n",
      "│    │    └─Conv2d: 3-103                (1,048,576)\n",
      "│    │    └─BatchNorm2d: 3-104           (1,024)\n",
      "│    │    └─Conv2d: 3-105                (2,359,296)\n",
      "│    │    └─BatchNorm2d: 3-106           (1,024)\n",
      "│    │    └─Conv2d: 3-107                (1,048,576)\n",
      "│    │    └─BatchNorm2d: 3-108           (4,096)\n",
      "│    │    └─ReLU: 3-109                  --\n",
      "│    └─Bottleneck: 2-16                  --\n",
      "│    │    └─Conv2d: 3-110                (1,048,576)\n",
      "│    │    └─BatchNorm2d: 3-111           (1,024)\n",
      "│    │    └─Conv2d: 3-112                (2,359,296)\n",
      "│    │    └─BatchNorm2d: 3-113           (1,024)\n",
      "│    │    └─Conv2d: 3-114                (1,048,576)\n",
      "│    │    └─BatchNorm2d: 3-115           (4,096)\n",
      "│    │    └─ReLU: 3-116                  --\n",
      "├─AdaptiveAvgPool2d: 1-9                 --\n",
      "├─Linear: 1-10                           4,098\n",
      "=================================================================\n",
      "Total params: 32,910,722\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 32,906,624\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vdqa1mh0tBee"
   },
   "outputs": [],
   "source": [
    "# test for inference\n",
    "import PIL.Image as Image\n",
    "import torchvision.transforms as transforms\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "do_test = 0\n",
    "if do_test:\n",
    "  for i in range(19):\n",
    "    if i==0:\n",
    "      im = Image.open('/content/drive/MyDrive/Dataset/Morteza/Data/Sample_Data/LungTest0/1/1.png')\n",
    "      im = im.resize([224,224])\n",
    "      image = to_tensor(im)\n",
    "    else:\n",
    "      # print('/content/data_sample/'+str(i+1)+'.png')\n",
    "      im = Image.open('/content/drive/MyDrive/Dataset/Morteza/Data/Sample_Data/LungTest0/1/'+str(i+1)+'.png')\n",
    "      im = im.resize([224,224])\n",
    "      next_channel = to_tensor(im)\n",
    "      image = torch.cat((image,next_channel),0);  \n",
    "  print(image.shape)\n",
    "  # image = torch.unsqueeze(image, 1)\n",
    "  uimage = torch.unsqueeze(image, 0)\n",
    "  model.eval()\n",
    "  out = model(uimage)\n",
    "  print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioB6Ma1VofAI",
    "outputId": "79ca8574-d223-4f95-f2d0-e72756c04ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "# Detect if we have a GPU available\n",
    "import torch.optim as optim\n",
    "model = model.to(device)\n",
    "params_to_update = model.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GnGniKwpLWn"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if is_inception and phase == 'train':                      \n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZHDJVvd4iDf"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from glob import glob\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3tlcLg0417_"
   },
   "outputs": [],
   "source": [
    "allExamples = glob(source_folder+\"/*/\", recursive = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "413EVJxl5DxI",
    "outputId": "623b8f51-d77e-43b0-bc32-d41e276f980c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openpyxl\n",
    "df = pd.read_excel(metal_indicator_lung)\n",
    "channel_names = df.Indicator.values\n",
    "co_marker = df.Comarker.values\n",
    "num_channels = len(channel_names)\n",
    "print(num_channels)\n",
    "allResponses = np.zeros((len(allExamples),num_channels*2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guYN7DvUAXu1"
   },
   "outputs": [],
   "source": [
    "def read_variable_file_check_examples(file_path,mother_examples):\n",
    "  df = pd.read_excel(file_path, dtype=str)\n",
    "  df_key_filename = pd.read_excel(keyFileNameCorr, dtype=str)\n",
    "  num_classes = len(df.columns.values)\n",
    "  counter = 0\n",
    "  aFNs = []\n",
    "  aFNLabels = []\n",
    "  for i in range(num_classes):\n",
    "    \n",
    "    cur_column = df[df.columns[i]].values\n",
    "    for key in cur_column:    \n",
    "      if type(key) is str and key!='nan':\n",
    "        \n",
    "        ind_num = [index for index, value in enumerate(df_key_filename[\"UniqueImageID\"].values) if value == key]\n",
    "        if len(ind_num)!=0:\n",
    "          counter = counter + 1        \n",
    "          ind_num = ind_num[0]\n",
    "          # print(key,', ',df_key_filename[\"FileName\"][ind_num])\n",
    "\n",
    "          existInExampleSet = -1\n",
    "          counter_eiE = 0\n",
    "          query_str = df_key_filename[\"FileName\"][ind_num]\n",
    "          while(existInExampleSet!=1) and counter_eiE < len(mother_examples):\n",
    "            if query_str in mother_examples[counter_eiE]:\n",
    "              existInExampleSet = 1\n",
    "            counter_eiE = counter_eiE + 1\n",
    "\n",
    "          if existInExampleSet==1:\n",
    "            aFNs.append(df_key_filename[\"FileName\"][ind_num])\n",
    "            aFNLabels.append(i)\n",
    "\n",
    "  return aFNs,aFNLabels\n",
    "\n",
    "\n",
    "aFileNames,aFileNamesLabels = read_variable_file_check_examples(clinical_var_address,allExamples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bM_B8QZqDN0a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "aFileNames_train, aFileNames_test, aFileNamesLabels_train, aFileNamesLabels_test = train_test_split(aFileNames, aFileNamesLabels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PreluEpDDfm6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "dir1 = dir_experiment+experiment_name\n",
    "dir2 = dir_experiment+experiment_name+\"/train/\"\n",
    "dir3 = dir_experiment+experiment_name+\"/val/\"\n",
    "\n",
    "used = set()\n",
    "uniqueLabels = [x for x in aFileNamesLabels if x not in used and (used.add(x) or True)]\n",
    "\n",
    "\n",
    "if not os.path.exists(dir_experiment):\n",
    "  os.mkdir(dir_experiment)\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(dir1):\n",
    "  os.mkdir(dir1)\n",
    "if not os.path.exists(dir2):\n",
    "  os.mkdir(dir2)\n",
    "if not os.path.exists(dir3):\n",
    "  os.mkdir(dir3)\n",
    "\n",
    "\n",
    "for j in range(len(uniqueLabels)):\n",
    "  c_dir = dir2+str(uniqueLabels[j])+'/'\n",
    "  if not os.path.exists(c_dir):\n",
    "    os.mkdir(c_dir)\n",
    "for j in range(len(uniqueLabels)):\n",
    "  c_dir = dir3+str(uniqueLabels[j])+'/'\n",
    "  if not os.path.exists(c_dir):\n",
    "    os.mkdir(c_dir)\n",
    "\n",
    "for i in range(len(aFileNames_train)):\n",
    "  file = dir2+str(aFileNamesLabels_train[i])+'/'+aFileNames_train[i]+'.tiff'\n",
    "  open(file, 'a').close()\n",
    "\n",
    "for i in range(len(aFileNames_test)):\n",
    "  file = dir3+str(aFileNamesLabels_test[i])+'/'+aFileNames_test[i]+'.tiff'\n",
    "  open(file, 'a').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jw4HU662FShE",
    "outputId": "d06fcfa6-5ff9-4173-dfd3-f67855000e12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def my_tiff_loader(filename):\n",
    "  head_tail = os.path.split(filename)\n",
    "  filenameWithTiff = head_tail[-1]\n",
    "  \n",
    "  fileNameToBeUsed = filenameWithTiff[0:-(len('.tiff'))]\n",
    "  folder_path = RAW_Image_Source_Folder+fileNameToBeUsed+'/'\n",
    "  for i in range(len(channel_names)):\n",
    "    if i==0:\n",
    "      im = Image.open(folder_path+channel_names[i]+'.png')\n",
    "      image = im.resize([224,224])\n",
    "      image = np.expand_dims(image,0).astype(np.float32)\n",
    "    else:\n",
    "      im = Image.open(folder_path+channel_names[i]+'.png')\n",
    "      next_channel = im.resize([224,224])\n",
    "      next_channel = np.expand_dims(next_channel,0).astype(np.float32)\n",
    "      image = np.concatenate((image,next_channel),0)\n",
    "\n",
    "  return image\n",
    "\n",
    "from torchvision import datasets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(dir1, x),loader=my_tiff_loader) for x in ['train', 'val']}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "multi_channel_resnet_arch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "25ea356dcdc9432eb8ead852c09391cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3481fd4957a84b3c950e7483d33e2ec2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55ecccbf86e3433e89a75b5b810e8a30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56990f43294f4b2dabd7a79c1d16ceb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25ea356dcdc9432eb8ead852c09391cc",
      "placeholder": "​",
      "style": "IPY_MODEL_d1b8b051dc944a5da7768a4fb409ce7c",
      "value": "100%"
     }
    },
    "78ee2f1b15fa41498075e1f58a5f1223": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55ecccbf86e3433e89a75b5b810e8a30",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3481fd4957a84b3c950e7483d33e2ec2",
      "value": 102530333
     }
    },
    "924cb40c28674d818e0048c266755f70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f572ff01bea48a8803018f146269d74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1b5d9e0a968445ab02188add8760207": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56990f43294f4b2dabd7a79c1d16ceb6",
       "IPY_MODEL_78ee2f1b15fa41498075e1f58a5f1223",
       "IPY_MODEL_b356474b77184c14b5002d3f27cc7317"
      ],
      "layout": "IPY_MODEL_bb226bb996a4492a93a96cba86e9ed13"
     }
    },
    "b356474b77184c14b5002d3f27cc7317": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f572ff01bea48a8803018f146269d74",
      "placeholder": "​",
      "style": "IPY_MODEL_924cb40c28674d818e0048c266755f70",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 167MB/s]"
     }
    },
    "bb226bb996a4492a93a96cba86e9ed13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1b8b051dc944a5da7768a4fb409ce7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
